<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Common sense reduced to calculus. More calculus. &middot; 90%Humour
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/archive/public/css/main.css">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/archive/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/archive/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_SVG"> </script>
  <script type="text/x-mathjax-config">
MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], ["\\(","\\)"] ], processEscapes: true } });
  </script>
  
</head>


  <body>

    <!-- Target for toggling the sidebar `.sidebar-checkbox` is for regular
     styles, `#sidebar-checkbox` for behavior. -->
<input type="checkbox" class="sidebar-checkbox" id="sidebar-checkbox">

<!-- Toggleable sidebar -->
<div class="sidebar" id="sidebar">
  
  <div class="sidebar-item">
    <div class="sidebar-personal-info">
  <!--
      <div class="sidebar-personal-info-section">
        <a href="http://gravatar.com/">
          <img src="http://www.gravatar.com/avatar/?s=350" title="View on Gravatar" alt="View on Gravatar" />
        </a>
      </div>
      <div class="sidebar-personal-info-section">
        <p>stories and musings from all walks of life.</p>
      </div>
      
  -->
    </div>
  </div>

  <nav class="sidebar-nav">
    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://90percenthumour.com">
          Home
        </a>

        
      </span>

    
      
      
      

      

      <span class="foldable">
        <a class="sidebar-nav-item " href="/archive/">
          Blog
        </a>

        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/archive/blog/categories/">
                Categories
              </a>
          
        
          
            
            
            
              <a class="sidebar-nav-item sidebar-nav-item-sub " href="/archive/blog/tags/">
                Tags
              </a>
          
        
      </span>

    
      
      
      

      

      <span class="">
        <a class="sidebar-nav-item " href="http://90percenthumour.com/about/">
          About
        </a>

        
      </span>

    

    <span class="sidebar-nav-item">Currently v1.0.0</span>
  </nav>

  <div class="sidebar-item">
    <p>
    &copy; 2017 90%Humour. This work is liscensed under <a href="http://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.
    </p>
  </div>

  
  <div class="sidebar-item">
    <p>
    Powered by <a href="http://jekyllrb.com">jekyll</a> </a>
    </p>
  </div>
</div>


    <!-- Wrap is the content to shift when toggling the sidebar. We wrap the
         content to avoid any CSS collisions with our real content. -->
    <div class="wrap">
      <div class="masthead">
        <div class="container">
          <h3 class="masthead-title">
            <a href="/archive/" title="Home" title="90%Humour">
              <img class="masthead-logo" src="/archive/public/logo.jpg"/>
            </a>
            <!--<small>stories and musings from all walks of life</small>-->
          </h3>
        </div>
      </div>

      <div class="container content">
        <div class="post">
  <h1 class="post-title">Common sense reduced to calculus. More calculus.</h1>
  <span class="post-date">10 Dec 2016</span>
   | 
  
  
  <article>
    <h3 id="where-does-the-story-begin">Where does the story begin?</h3>

<p>Every day, on numerous occasions, we tend to take decisions based on opinions. But this is not a point of dismay, becuase contrary to our speculation, reasoning with assumptions can be achieved within the elegance of just two guiding rules - the sum and product rule of probability theory. Assumptions, opinions, approximate knowledge or beliefs - they are merely placeholders when viewed through the mathematical construct of uncertainty. Uncertainty (or equivalently degree of belief) in the variability of <script type="math/tex">x</script>, is quantified as a real number through a function <script type="math/tex">p(x)</script>, more formally defined as probability. This function, by virtue of Cox’s axioms must uphold the rules of probability theory, in particular, the sum and product rule.</p>

<p>Sum-rule:</p>

<script type="math/tex; mode=display">P(AB) + P(\bar{A}|B) = 1</script>

<p>Product-rule:</p>

<script type="math/tex; mode=display">P(AB|C) = P(A|BC)P(BC)</script>

<h3 id="how-does-this-play-with-ml-or-ai">How does this play with ML or AI?</h3>

<p>Uncertainty has an all-pervasive presence in a statistical model - from noise in the observations, to a model’s belief about its parameters, to uncertainty in model predictions, to our belief about the model itself. But with its explicit and faithful representation in Bayesian probability theory, one can reason with it in a principled way.</p>

<p>Most ML tasks can be conveniently (and crudely) summarised as follows - a model, accompanied with its explicit and necessary assumptions based on prior knowledge, is capable of making predictions and performing inference in the light of some observations. In this simplistic pipeline, it is easy to examine how uncertainty factors at every step of modelling.</p>

<p><strong>Uncertainty representation</strong>: All the the believed uncertainties in the mathematical description of a model are stated via functional definitions - prior <script type="math/tex">p(\theta\vert m)</script> , likelihood <script type="math/tex">p(D\vert\theta,m)</script>, noise processes, among other expressions. These functional expressions and implcit conditioning, capture different conceptions of uncertainties like parametric and structural uncertainties.</p>

<p><strong>Uncertainty propagation</strong>: All the different kinds of uncertainties, implicit or explicit in an model should be propagated for decision-making (computing utilities for prediction or parameter-setting), which in general translates to prediction or inference. Mathematically, this involves the use of marginalisation principle (a direct application of sum and product rule).</p>

<p><strong>Inference and Prediction</strong>: The task of inference or learning boils down to squeezing prior uncertainties <script type="math/tex">(p(\theta))</script>, (often with respect to parameters, latent variables etc.) through the data, <script type="math/tex">D</script>, to posterior uncertainties <script type="math/tex">p(\theta\vert D,m)</script> (following Bayes’ rule).</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(\theta|D,m) = \frac{p(D|\theta,m)p(\theta|m)}{p(D|m)} && p(D|m) = \int p(D|\theta,m)p(\theta|m)\mathrm{d}\theta
\end{align} %]]></script>

<p>The normalisation constant, <script type="math/tex">p(D\vert m)</script>, is referred to as <em>Marginal Likelihood</em> or <em>Model Evidence</em>. At the level of conditioning on <script type="math/tex">m</script>, Bayesian model selection is naturally implemented as Occam’s Razor. Moreover, these updated uncertainties in the posterior can be sequentially propagated while making predictions about the unobserved (new data, <script type="math/tex">D^*</script>).</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(D^*|D,m) &= \int p(D^*,\theta\vert D,m)\mathrm{d}\theta\\
           &= \int p(D^*\vert\theta,D,m) p(\theta\vert D,m) \mathrm{d}\theta
           &= \int p(D^*\vert\theta,m) p(\theta\vert D,m) \mathrm{d}\theta
\end{align} %]]></script>

<h3 id="logistics-can-be-tricky">Logistics can be tricky</h3>

<p>The elegance of first principles makes Bayesian reasoning extremely compelling. However, the theory merely guides on what-to-do and often fails on the how-to front. This latter one is where the practicality and feasibility are contested, or in other words computational and analytical intractability. And more often then what one would desire, these computations are intractable. Therefore, in practice a range of approximations are used. The inexpensive point estimates, being the most common ones where one settles for solutions from optimisation of maximum likelihood and maximum a posteriori. And then there are  deterministic approximations which harness structural understanding of the model like Variational mean field and Expectation Propagation.  Lastly, there are the computationally demanding but often asymptotically exact stochastic methods (like sampling).</p>

<h3 id="common-sense-reduced-to-calculus-more-calculus">Common sense reduced to calculus. More calculus.</h3>

<p>Variational Inference appends to the set of model assumptions, a parameterised approximate posterior ,<script type="math/tex">q_{\lambda}(\theta)</script>. Thus, it encourages you to search for the posterior in a constrained space of possible choices. A space constrained by the choice of paramteric form and, spanned by the range of variational parameters. More importantly, the choice of approximate posterior is not arbitrary. The theory proposes another guiding principle - choose a distribution that most similar the exact posterior of the original model. Similarity, here, being measured via  Kullback–Leibler divergence (KL-divergence), <script type="math/tex">\mathcal{D}_{KL}</script>, which is defined as follows,</p>

<script type="math/tex; mode=display">\mathcal{D}_{KL}(q_{\lambda}(\theta)\vert\vert p(\theta\vert D,m)) \equiv \int q_{\lambda}(\theta)\log \frac{p(\theta\vert D,m)}{q(\theta)}\mathrm{d}\theta</script>

<p>Thus, the VI approach can be summaried as minimising</p>

<script type="math/tex; mode=display">q_{\lambda^*}(\theta) = \text{argmin}  \mathcal{D}_{KL}(q_{\lambda}(\theta)\vert\vert p(\theta\vert D,m))</script>

<p>Moreover, the obtained approximate-posterior can replace the exact posterior for any further practical compuatations, like  prediction,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
&& q(D^*|D,m) = \int p(D^*\vert\theta,m) q_{\lambda^*}(\theta\vert D,m) \mathrm{d}\theta
\end{align} %]]></script>

<p>The obvious point of contention here is, why this particular choice of similarity measure? And what are its qualitiative properties.</p>

<h4 id="why-this-similarity-measure">Why this similarity measure?</h4>

<p>This similarity measure naturally yields a lower bound for the marginal likelihood. A quantity, we would have maximised for during model selection. Thus, (hand-wavyly) we are assured that we are not very far from the ‘best-model’ search we had foresaken due to intractabilities.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\mathcal{D}_{KL}(q_{\lambda}(\theta)\vert\vert p(\theta\vert D,m)) &\equiv \int q_{\lambda}(\theta)\log \frac{q_{\lambda}(\theta)}{p(\theta\vert D,m)}\mathrm{d}\theta\\
                 &= \int q_{\lambda}(\theta)\log \frac{q_{\lambda}(\theta)p(D\vert m)}{p(\theta, D\vert m)}\mathrm{d}\theta\\
                 &= \int q_{\lambda}(\theta)\log \frac{q_{\lambda}(\theta)}{p(\theta, D\vert m)}\mathrm{d}\theta + \int q_{\lambda}(\theta)\log p(D\vert m) \mathrm{d}\theta\\
                 &= \int q_{\lambda}(\theta)\log \frac{q_{\lambda}(\theta)}{p(\theta, D\vert m)}\mathrm{d}\theta + \log p(D\vert m)
\end{align} %]]></script>

<p>equivalently,</p>

<script type="math/tex; mode=display">\log p(D\vert m) =   \int q_{\lambda}(\theta)\log \frac{p(\theta, D\vert m)}{q_{\lambda}(\theta)}\mathrm{d}\theta + {D}_{KL}(q_{\lambda}(\theta)\vert\vert p(\theta\vert D,m))</script>


  </article>
</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/archive/travel/2017/03/13/chadar/">
            Chadar Trek Video
            <small>13 Mar 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/archive/ml/2017/02/23/exploring-gaussians/">
            Exploring Gaussians
            <small>23 Feb 2017</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/archive/ml/2017/02/07/generative/">
            Create to understand
            <small>07 Feb 2017</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>


<div class="comments">
  <h2>Comments</h2>
  <div id="disqus_thread"></div>
  <script>
var disqus_config = function () {
  this.page.url = 'http://localhost:4000/ml/2016/12/10/vfe/'; // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = '/ml/2016/12/10/vfe'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() {
  var d = document, s = d.createElement('script');
  s.src = '//ambrishrawat.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
})();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</div>


      </div>
    </div>

    <label for="sidebar-checkbox" class="sidebar-toggle"></label>

    <script>
      (function(document) {
        var toggle = document.querySelector('.sidebar-toggle');
        var sidebar = document.querySelector('#sidebar');
        var checkbox = document.querySelector('#sidebar-checkbox');

        document.addEventListener('click', function(e) {
          var target = e.target;

          if (target === toggle) {
            checkbox.checked = !checkbox.checked;
            e.preventDefault();
          } else if (checkbox.checked && !sidebar.contains(target)) {
            /* click outside the sidebar when sidebar is open */
            checkbox.checked = false;
          }
        }, false);
      })(document);
    </script>
    
    <script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-00000000-1', 'auto');
ga('send', 'pageview');
    </script>
    
  </body>
  
  <script id="dsq-count-scr" src="//ambrishrawat.disqus.com/count.js" async></script>
  
</html>
